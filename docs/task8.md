# Задание 8 — Выявление и устранение «горячих» шардов MongoDB

Ситуация: из-за популярной категории **«Электроника»** один шард перегружен (≈70% запросов), что приводит к росту latency, очередям и деградации приложения. Нужно:
1) метрики для раннего выявления дисбаланса;
2) механизмы автоматического перераспределения нагрузки/данных.

Документ ориентирован на MongoDB Sharded Cluster (mongos + configRS + shardRS).

---

## 1) Почему возникает “горячий” шард

Типовые причины:
- **Плохой shard key**: ключ коррелирует с популярностью (например, `category` или `geo_zone`) → один shard получает непропорционально много запросов и/или записей.
- **Range sharding + монотонный ключ** (время/инкремент) → “горячие” чанки на конце диапазона.
- **Неравномерные чанки** (jumbo chunks, неудачный split) → часть данных физически сконцентрирована.
- **Read hotspot**: даже при равномерном распределении данных кэш/индексы/паттерн запросов создают дисбаланс IOPS на конкретных нодах.
- **Balancing lag**: балансировщик не успевает, выключен или ограничен окнами.

---

## 2) Метрики мониторинга (что собирать)

Ниже — минимальный набор метрик, которые позволяют:
- увидеть перегрев по шард-узлам/репликам,
- понять “почему” (дисбаланс по запросам, по данным, по очередям, по блокировкам),
- принять решение: масштабировать, перебалансировать, менять shard key/модель.

### 2.1. Метрики на уровне mongos (роутер)

**Цель:** понять, куда реально уходит нагрузка и сколько запросов scatter-gather.

1) **Requests per shard (rate)**  
- количество запросов, проксируемых mongos, по каждому шард-идентификатору  
- признак перегрева: один шард стабильно > X% (например, >50%) при сопоставимом объёме данных

2) **Scatter-gather rate**  
- доля запросов, которые обращаются к нескольким шардам  
- рост scatter-gather усиливает нагрузку на весь кластер

3) **Latency per operation class**  
- p50/p95/p99 для: `find`, `aggregate`, `update`, `insert`, `getMore`  
- отдельно по namespace (`db.collection`)

4) **Connection pool / queueing**  
- число активных соединений к shards  
- длины очередей/ожиданий (если наблюдаете “подвисания” на mongos)

**Команды/точки данных (для ручной диагностики):**
```js
// 1) общий статус шардинга + распределение чанков
sh.status()

// 2) распределение конкретной коллекции
use somedb
db.products.getShardDistribution()
````

---

### 2.2. Метрики на уровне shard (каждый RS и каждая нода)

**Цель:** увидеть, что именно “вырезает” производительность на перегретом шарде.

1. **CPU / iowait / load average** (узел)

* перегрев: CPU близко к 100% или высокий iowait при росте latency

2. **Disk IO / latency** (узел)

* read/write IOPS, throughput, p95 latency
* перегрев: рост p95 latency диска на shard primary

3. **WiredTiger cache**

* used %, eviction rate, pages read into cache
* перегрев: постоянные эвикшены и cache pressure → деградация чтений

4. **opcounters / opcountersRepl**

* rate операций `query`, `getmore`, `insert`, `update`, `delete`, `command`
* сравнение по шардам: “горячий” шард сильно выделяется

5. **Locks / queueing / tickets**

* время ожидания блокировок, WT tickets (read/write tickets), количество ожиданий

6. **Replication lag (для RS)**

* `optimeDate` primary vs secondary
* перегрев: рост lag → secondary не успевают, растёт риск stale reads/rollback

7. **Размер данных и чанков**

* docs/data per shard, кол-во чанков, jumbo chunks
* признак: один шард содержит значительно больше/меньше данных, чем остальные (или jumbo не двигаются)

**Команды для диагностики:**

```js
// На primary конкретного шарда (shardX-1)
db.serverStatus()

// Репликация
rs.status()

// Топ “тяжёлых” операций (включать на время расследования)
db.currentOp({ active: true })

// Профилирование (точечно и кратковременно)
db.setProfilingLevel(1, { slowms: 100 })
db.system.profile.find().sort({ ts: -1 }).limit(20)
```

---

### 2.3. Метрики балансировщика и миграций чанков

**Цель:** понять, движутся ли чанки и успевает ли система самовосстанавливаться.

1. **Balancer state**

* включен/выключен, активен/не активен

2. **Chunk migrations rate + duration**

* сколько миграций/мин, среднее/95p время миграции
* ошибки миграции

3. **Auto-split events**

* частота split’ов, наличие jumbo chunks

**Команды:**

```js
// Балансировщик
sh.getBalancerState()
sh.isBalancerRunning()

// Проверка настроек (окна балансировки)
use config
db.settings.find()

// Логи миграций (исторически)
use config
db.changelog.find({ what: /moveChunk|split/ }).sort({ time: -1 }).limit(50)
```

---

### 2.4. Алёрты (пороговые правила)

Минимальные практичные алёрты:

* **Shard traffic skew**: один шард > 50% QPS 10–15 минут
* **Shard latency**: p95 на shard primary > X ms (например, >50–100ms) 5–10 минут
* **Replication lag**: lag > 2–5 сек (для критичных коллекций) 5 минут
* **Disk p95 latency** > X ms или iowait > X%
* **WiredTiger eviction storm**: eviction rate/сек превышает baseline
* **Balancer disabled** или **миграции падают** N раз подряд
* **Jumbo chunks detected** (чанк не может мигрировать)

---

## 3) Механизмы устранения дисбаланса (что делать)

Ниже — набор мер “от быстро-временных” к “структурным”.

---

## 3.1. Быстрые меры (оперативная стабилизация)

### A) Включить/проверить балансировщик

```js
sh.setBalancerState(true)
sh.isBalancerRunning()
```

Если у вас балансировщик ограничен окнами:

```js
use config
db.settings.updateOne(
  { _id: "balancer" },
  { $set: { activeWindow: { start: "00:00", stop: "23:59" } } },
  { upsert: true }
)
```

### B) Принудительно “разрезать” горячие диапазоны (если range sharding)

Если shard key range-типа (например, `category` или `created_at`), можно делать **ручные split** вокруг горячих значений (временно).

```js
// пример: split по ключу (условно)
sh.splitAt("somedb.products", { category: "electronics" })
```

Далее — миграция конкретных чанков на другие шарды:

```js
// moveChunk требует указания ключа, попадающего в нужный чанк
sh.moveChunk("somedb.products", { category: "electronics" }, "shard2")
```

Ограничение: если `category` как shard key — “электроника” будет стремиться снова нагреться из-за запросов, даже если данные частично переедут.

### C) Read scaling через secondary (если допустимо)

Если для части запросов допустима небольшая устарелость, разгружаем primary:

* выставляем `readPreference=secondaryPreferred` для некритичных чтений (подробнее в задании 9)
* следим за lag (чтобы не ухудшить UX)

---

## 3.2. Автоматическое перераспределение нагрузки (рекомендуемая стратегия)

Ключевая мысль: **автоматически мигрировать “горячие чанки” и/или менять распределение ключей** на основании метрик.

### Вариант 1 — Авто-перенос “горячих” чанков (chunk-level hotness)

1. собираем статистику запросов по чанкам/диапазонам shard key (через профилирование/FTDC/логирование запросов на mongos);
2. определяем “hot chunks” (верхний 1–5% по QPS/latency contribution);
3. запускаем контроллер, который:

   * инициирует `split` горячих чанков (если они крупные),
   * делает `moveChunk` на менее нагруженные шарды,
   * соблюдает rate-limit миграций (чтобы не убить кластер).

**Псевдо-алгоритм:**

* если shard QPS > threshold и skew > threshold:

  * найти top-keys/chunks в этой коллекции
  * split hot chunk на меньшие
  * moveChunk на shard с минимальным QPS/CPU

**Пример команд (ручной запуск, шаблон):**

```js
// 1) разрезать горячий чанк
sh.splitAt("somedb.products", { <shardKey>: <splitPoint> })

// 2) перенести часть на другой shard
sh.moveChunk("somedb.products", { <shardKey>: <valueInsideChunk> }, "shard2")
```

Ограничение: если shard key “плохой” (например, `category`) — перегрев будет возвращаться, потому что запросы будут продолжать таргетировать одну область ключа.

---

### Вариант 2 — Изменение shard key / модель данных (структурное решение)

Если причина перегрева — **сам shard key** (например, `category`), правильное решение: **перейти на ключ, который распределяет нагрузку равномерно**.

Для `products` типичный выбор:

* `{ _id: "hashed" }` (точечные операции по карточке/остаткам — 1-shard)
* каталог и фильтры обслуживать через поисковый слой/кэш/read-модель

**Практический путь миграции shard key:**

* MongoDB не “меняет shard key на лету” просто так для существующей коллекции (в зависимости от версии — есть процедуры/ограничения).
* Надёжный путь: создать новую коллекцию (например, `products_v2`) и перелить данные.

**Шаблон:**

```js
// создать новую коллекцию с новым shard key
sh.shardCollection("somedb.products_v2", { _id: "hashed" })

// перелив данных (примерно; способ зависит от пайплайна миграции)
db.products.aggregate([{ $match: {} }, { $out: "products_v2" }])
```

Далее переключить приложение на `products_v2` (или через view/alias на уровне приложения).

---

### Вариант 3 — “Соление” ключа (key salting) для популярных значений

Если бизнес-ключ необходим (например, категоризация), но есть перекос, можно ввести **salt**:

* вместо shard key `category`, сделать `category_salt = category + "#" + (hash(product_id) % N)`
* shard key: `{ category_salt: 1 }` или `{ category_salt: "hashed" }`

Плюсы:

* “electronics” размазывается по N bucket’ам, уходя от одного шарда.

Минусы:

* запрос по “electronics” требует либо:

  * fan-out по N salt-значениям, либо
  * дополнительного индексного слоя.

---

### Вариант 4 — Зональное шардирование (zone sharding) по географии

Если причина в геозоне (а не в категории), применяют **Zones**:

* geo_zone → закрепить за подмножеством шардов
* для “электроники” это не решает проблему, если hotspot по категории глобальный.

---

## 3.3. Сценарий “электроника перегружает шард” — рекомендуемое решение

1. **Диагностика**

* подтверждаем, что перегрев вызван именно запросами к `products` по `category=electronics`:

  * профилирование на mongos + shard primary
  * анализ топ-запросов и доли “electronics”

2. **Оперативно**

* включить балансировщик, убедиться что нет jumbo chunks
* временно split/move (если помогает) + усилить кэш (Redis/CDN/edge) на каталог

3. **Структурно**

* отказаться от shard key по `category`
* перейти для `products` на `{ _id: "hashed" }`
* каталог/поиск вынести в search/read-модель (или хотя бы агрессивный кэш с TTL)

---

## 4) Минимальные примеры команд для ревью (MongoDB)

### 4.1. Проверка распределения коллекции и шардов

```js
use somedb
db.products.getShardDistribution()
db.orders.getShardDistribution()
db.carts.getShardDistribution()
```

### 4.2. Проверка балансировщика и истории миграций

```js
sh.getBalancerState()
sh.isBalancerRunning()

use config
db.changelog.find({ what: /moveChunk|split/ }).sort({ time: -1 }).limit(20)
```

### 4.3. Поиск jumbo chunks (которые “не двигаются”)

```js
use config
db.chunks.find({ ns: "somedb.products", jumbo: true })
```

### 4.4. Пример split и moveChunk (шаблон)

```js
// split (пример — зависит от shard key)
sh.splitAt("somedb.products", { <shardKey>: <splitPoint> })

// moveChunk (пример — зависит от shard key)
sh.moveChunk("somedb.products", { <shardKey>: <valueInsideChunk> }, "shard2")
```

---

## 5) Итог

**Метрики**, которые обязательно нужны:

* QPS/latency по shard’ам (mongos + shard primaries)
* replication lag
* disk/io + WiredTiger cache pressure
* баланcировщик: включен/работает/ошибки миграций
* распределение чанков + jumbo chunks

**Авто-меры**, которые реально работают:

* hot chunk detection → split → moveChunk (rate-limited)
* отказ от shard key, коррелирующего с популярностью (category) → hashed по `_id`
* key salting для популярных значений, если нужно шардировать по бизнес-атрибуту
* read scaling/кэширование для каталога, чтобы не “стрелять” в MongoDB всем трафиком
